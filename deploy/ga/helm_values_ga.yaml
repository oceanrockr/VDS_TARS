# =====================================================================
# T.A.R.S. v1.0.0 GA - Production Helm Values
# =====================================================================
# This file contains production-grade Helm values for T.A.R.S. v1.0.0 GA.
# Values are based on Phase 13.8 benchmark results and production readiness
# validation.
#
# Resource settings are tuned for:
#   - 50 RPS sustained throughput
#   - <300s evaluation latency (p95)
#   - <100ms hot-reload latency (p95)
#   - <50ms API response time (p95)
#   - 99.9% availability SLO
#
# Prerequisites:
#   - Kubernetes 1.28+
#   - Prometheus Operator
#   - cert-manager
#   - Secrets pre-deployed (JWT, DB, TLS)
# =====================================================================

# =====================================================================
# GLOBAL SETTINGS
# =====================================================================
global:
  # Environment
  environment: production
  region: us-east-1

  # Image settings
  imageRegistry: ghcr.io
  imageOrganization: veleron-dev-studios
  imagePullPolicy: IfNotPresent
  imagePullSecrets:
    - name: ghcr-pull-secret

  # Default replica count (overridden per service)
  replicaCount: 3

  # Security context (non-root, read-only filesystem)
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    runAsGroup: 3000
    fsGroup: 2000
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    capabilities:
      drop:
        - ALL

  # Node affinity (production nodes only)
  nodeSelector:
    environment: production
    workload-type: cpu-optimized

  tolerations:
    - key: "production"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"

  # Pod anti-affinity (spread across nodes)
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/part-of: tars
            topologyKey: kubernetes.io/hostname

# =====================================================================
# EVALUATION ENGINE
# =====================================================================
evalEngine:
  enabled: true
  name: eval-engine
  image:
    repository: ghcr.io/veleron-dev-studios/tars-eval-engine
    tag: v1.0.0

  replicaCount: 3

  # Resource limits (based on Phase 13.8 benchmarks)
  # 50 RPS requires ~1.5 CPU, 3 GB RAM per pod
  resources:
    requests:
      cpu: 1500m
      memory: 3Gi
    limits:
      cpu: 2000m
      memory: 4Gi

  # Autoscaling (HPA)
  autoscaling:
    enabled: true
    minReplicas: 3
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70
    targetMemoryUtilizationPercentage: 80
    behavior:
      scaleDown:
        stabilizationWindowSeconds: 300
        policies:
          - type: Percent
            value: 50
            periodSeconds: 60
      scaleUp:
        stabilizationWindowSeconds: 60
        policies:
          - type: Percent
            value: 100
            periodSeconds: 30

  # Pod Disruption Budget
  podDisruptionBudget:
    enabled: true
    minAvailable: 2

  # Service
  service:
    type: ClusterIP
    port: 8080
    targetPort: 8080

  # Health checks
  livenessProbe:
    httpGet:
      path: /health
      port: 8080
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3

  readinessProbe:
    httpGet:
      path: /ready
      port: 8080
    initialDelaySeconds: 15
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 3

  # Environment variables
  env:
    - name: LOG_LEVEL
      value: "INFO"
    - name: LOG_FORMAT
      value: "json"
    - name: REDIS_HOST
      value: "redis-master.tars-data.svc.cluster.local"
    - name: REDIS_PORT
      value: "6379"
    - name: POSTGRES_HOST
      value: "postgres-primary.tars-data.svc.cluster.local"
    - name: POSTGRES_PORT
      value: "5432"
    - name: POSTGRES_DB
      value: "tars_production"
    - name: POSTGRES_MAX_CONNECTIONS
      value: "100"
    - name: JAEGER_AGENT_HOST
      value: "jaeger-agent.observability.svc.cluster.local"
    - name: JAEGER_AGENT_PORT
      value: "6831"
    - name: JAEGER_SAMPLER_TYPE
      value: "probabilistic"
    - name: JAEGER_SAMPLER_PARAM
      value: "0.1"
    - name: OTEL_EXPORTER_OTLP_ENDPOINT
      value: "http://otel-collector.observability.svc.cluster.local:4318"

  # Secrets (mounted as env vars)
  secrets:
    - name: POSTGRES_USER
      secretName: tars-postgres-credentials
      secretKey: username
    - name: POSTGRES_PASSWORD
      secretName: tars-postgres-credentials
      secretKey: password
    - name: REDIS_PASSWORD
      secretName: tars-redis-credentials
      secretKey: password
    - name: JWT_SECRET
      secretName: tars-jwt-secret
      secretKey: secret

  # Prometheus metrics
  metrics:
    enabled: true
    port: 9090
    path: /metrics
    serviceMonitor:
      enabled: true
      interval: 15s
      scrapeTimeout: 10s

  # Sync wave (ArgoCD)
  annotations:
    argocd.argoproj.io/sync-wave: "1"

# =====================================================================
# HYPERSYNC SERVICE
# =====================================================================
hyperSync:
  enabled: true
  name: hypersync
  image:
    repository: ghcr.io/veleron-dev-studios/tars-hypersync
    tag: v1.0.0

  replicaCount: 3

  resources:
    requests:
      cpu: 750m
      memory: 1.5Gi
    limits:
      cpu: 1000m
      memory: 2Gi

  autoscaling:
    enabled: true
    minReplicas: 3
    maxReplicas: 8
    targetCPUUtilizationPercentage: 70

  podDisruptionBudget:
    enabled: true
    minAvailable: 2

  service:
    type: ClusterIP
    port: 8098
    targetPort: 8098

  livenessProbe:
    httpGet:
      path: /health
      port: 8098
    initialDelaySeconds: 20
    periodSeconds: 10

  readinessProbe:
    httpGet:
      path: /ready
      port: 8098
    initialDelaySeconds: 10
    periodSeconds: 5

  env:
    - name: LOG_LEVEL
      value: "INFO"
    - name: REDIS_HOST
      value: "redis-master.tars-data.svc.cluster.local"
    - name: APPROVAL_MODE
      value: "threshold"
    - name: APPROVAL_THRESHOLD
      value: "5.0"
    - name: SYNC_INTERVAL
      value: "30"

  metrics:
    enabled: true
    port: 9090

  annotations:
    argocd.argoproj.io/sync-wave: "1"

# =====================================================================
# ORCHESTRATION AGENT
# =====================================================================
orchestration:
  enabled: true
  name: orchestration
  image:
    repository: ghcr.io/veleron-dev-studios/tars-orchestration
    tag: v1.0.0

  replicaCount: 3

  resources:
    requests:
      cpu: 500m
      memory: 1Gi
    limits:
      cpu: 1000m
      memory: 2Gi

  autoscaling:
    enabled: true
    minReplicas: 3
    maxReplicas: 6
    targetCPUUtilizationPercentage: 70

  podDisruptionBudget:
    enabled: true
    minAvailable: 2

  service:
    type: ClusterIP
    port: 8094
    targetPort: 8094

  livenessProbe:
    httpGet:
      path: /health
      port: 8094
    initialDelaySeconds: 20
    periodSeconds: 10

  readinessProbe:
    httpGet:
      path: /ready
      port: 8094
    initialDelaySeconds: 10
    periodSeconds: 5

  metrics:
    enabled: true
    port: 9090

  annotations:
    argocd.argoproj.io/sync-wave: "1"

# =====================================================================
# RL AGENTS (DQN, A2C, PPO, DDPG)
# =====================================================================
agents:
  enabled: true

  # Common settings for all agents
  common: &agent-common
    replicaCount: 2
    resources:
      requests:
        cpu: 500m
        memory: 1Gi
      limits:
        cpu: 1000m
        memory: 2Gi
    autoscaling:
      enabled: true
      minReplicas: 2
      maxReplicas: 5
      targetCPUUtilizationPercentage: 75
    podDisruptionBudget:
      enabled: true
      minAvailable: 1
    metrics:
      enabled: true
      port: 9090
    annotations:
      argocd.argoproj.io/sync-wave: "2"

  # DQN Agent
  dqn:
    <<: *agent-common
    name: agent-dqn
    image:
      repository: ghcr.io/veleron-dev-studios/tars-agent-dqn
      tag: v1.0.0
    service:
      port: 8100
      targetPort: 8100

  # A2C Agent
  a2c:
    <<: *agent-common
    name: agent-a2c
    image:
      repository: ghcr.io/veleron-dev-studios/tars-agent-a2c
      tag: v1.0.0
    service:
      port: 8101
      targetPort: 8101

  # PPO Agent
  ppo:
    <<: *agent-common
    name: agent-ppo
    image:
      repository: ghcr.io/veleron-dev-studios/tars-agent-ppo
      tag: v1.0.0
    service:
      port: 8102
      targetPort: 8102

  # DDPG Agent
  ddpg:
    <<: *agent-common
    name: agent-ddpg
    image:
      repository: ghcr.io/veleron-dev-studios/tars-agent-ddpg
      tag: v1.0.0
    service:
      port: 8103
      targetPort: 8103

# =====================================================================
# DASHBOARD API
# =====================================================================
dashboardApi:
  enabled: true
  name: dashboard-api
  image:
    repository: ghcr.io/veleron-dev-studios/tars-dashboard-api
    tag: v1.0.0

  replicaCount: 3

  resources:
    requests:
      cpu: 500m
      memory: 1Gi
    limits:
      cpu: 1000m
      memory: 2Gi

  autoscaling:
    enabled: true
    minReplicas: 3
    maxReplicas: 8
    targetCPUUtilizationPercentage: 70

  podDisruptionBudget:
    enabled: true
    minAvailable: 2

  service:
    type: ClusterIP
    port: 3001
    targetPort: 3001

  livenessProbe:
    httpGet:
      path: /health
      port: 3001
    initialDelaySeconds: 15
    periodSeconds: 10

  readinessProbe:
    httpGet:
      path: /ready
      port: 3001
    initialDelaySeconds: 10
    periodSeconds: 5

  env:
    - name: NODE_ENV
      value: "production"
    - name: LOG_LEVEL
      value: "info"
    - name: REDIS_HOST
      value: "redis-master.tars-data.svc.cluster.local"
    - name: CORS_ORIGIN
      value: "https://dashboard.tars.prod"

  metrics:
    enabled: true
    port: 9090

  annotations:
    argocd.argoproj.io/sync-wave: "3"

# =====================================================================
# DASHBOARD FRONTEND
# =====================================================================
dashboardFrontend:
  enabled: true
  name: dashboard-frontend
  image:
    repository: ghcr.io/veleron-dev-studios/tars-dashboard-frontend
    tag: v1.0.0

  replicaCount: 3

  resources:
    requests:
      cpu: 200m
      memory: 512Mi
    limits:
      cpu: 500m
      memory: 1Gi

  autoscaling:
    enabled: true
    minReplicas: 3
    maxReplicas: 6
    targetCPUUtilizationPercentage: 70

  podDisruptionBudget:
    enabled: true
    minAvailable: 2

  service:
    type: ClusterIP
    port: 3000
    targetPort: 80

  livenessProbe:
    httpGet:
      path: /
      port: 80
    initialDelaySeconds: 10
    periodSeconds: 10

  readinessProbe:
    httpGet:
      path: /
      port: 80
    initialDelaySeconds: 5
    periodSeconds: 5

  env:
    - name: REACT_APP_API_URL
      value: "https://api.tars.prod"
    - name: REACT_APP_ENV
      value: "production"

  annotations:
    argocd.argoproj.io/sync-wave: "3"

# =====================================================================
# AUTHENTICATION & AUTHORIZATION
# =====================================================================
auth:
  jwt:
    enabled: true
    algorithm: HS256
    accessTokenExpiry: 3600  # 1 hour
    refreshTokenExpiry: 604800  # 7 days

  rbac:
    enabled: true
    roles:
      - viewer
      - developer
      - admin

  rateLimiting:
    enabled: true
    rps: 50
    burstSize: 100
    backend: redis

# =====================================================================
# TLS / mTLS
# =====================================================================
tls:
  enabled: true
  issuer: letsencrypt-prod
  secretName: tars-tls-cert

mtls:
  enabled: true
  caCertSecretName: tars-mtls-ca
  clientCertSecretName: tars-mtls-client

# =====================================================================
# INGRESS
# =====================================================================
ingress:
  enabled: true
  className: nginx
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
    nginx.ingress.kubernetes.io/rate-limit: "50"
    nginx.ingress.kubernetes.io/limit-rps: "50"
    nginx.ingress.kubernetes.io/limit-connections: "100"
    nginx.ingress.kubernetes.io/proxy-body-size: "10m"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "300"

  hosts:
    - host: api.tars.prod
      paths:
        - path: /
          pathType: Prefix
          service: dashboard-api
          port: 3001

    - host: dashboard.tars.prod
      paths:
        - path: /
          pathType: Prefix
          service: dashboard-frontend
          port: 3000

  tls:
    - secretName: tars-tls-cert
      hosts:
        - api.tars.prod
        - dashboard.tars.prod

  annotations:
    argocd.argoproj.io/sync-wave: "3"

# =====================================================================
# DATA STORES
# =====================================================================
postgresql:
  enabled: true
  host: postgres-primary.tars-data.svc.cluster.local
  port: 5432
  database: tars_production
  maxConnections: 100
  poolSize: 20
  connectionTimeout: 5000
  idleTimeout: 600000

  # High availability
  replication:
    enabled: true
    synchronousCommit: remote_apply
    maxStandbyDelay: 3000  # 3s

  # Backup
  backup:
    enabled: true
    schedule: "0 2 * * *"  # Daily at 2am UTC
    retention: 30  # 30 days

  # Monitoring
  metrics:
    enabled: true
    exporter: postgres_exporter

redis:
  enabled: true
  host: redis-master.tars-data.svc.cluster.local
  port: 6379
  database: 0
  maxMemory: 4gb
  maxMemoryPolicy: allkeys-lru

  # Persistence
  persistence:
    enabled: true
    rdb:
      enabled: true
      schedule: "*/15 * * * *"  # Every 15 min
    aof:
      enabled: true
      appendFsync: everysec

  # High availability
  sentinel:
    enabled: true
    quorum: 2

  # Monitoring
  metrics:
    enabled: true
    exporter: redis_exporter

# =====================================================================
# OBSERVABILITY
# =====================================================================
prometheus:
  enabled: true
  scrapeInterval: 15s
  scrapeTimeout: 10s
  evaluationInterval: 15s
  retention: 30d

  serviceMonitors:
    - evalEngine
    - hyperSync
    - orchestration
    - agentDqn
    - agentA2c
    - agentPpo
    - agentDdpg
    - dashboardApi

grafana:
  enabled: true
  dashboards:
    - tars-overview
    - tars-eval-engine
    - tars-hypersync
    - tars-agents
    - tars-slo
    - tars-multi-region
    - tars-security
    - tars-cost

jaeger:
  enabled: true
  samplingRate: 0.1  # 10% sampling
  collector:
    host: jaeger-collector.observability.svc.cluster.local
    port: 14268

opentelemetry:
  enabled: true
  endpoint: http://otel-collector.observability.svc.cluster.local:4318
  protocol: http/protobuf

logging:
  level: INFO
  format: json
  structuredLogging: true
  fields:
    - timestamp
    - level
    - service
    - trace_id
    - span_id
    - message
    - error

# =====================================================================
# SLO / SLA TARGETS
# =====================================================================
slo:
  # Evaluation latency (p95)
  evaluationLatencyP95: 300s
  evaluationLatencyP99: 600s

  # Hot-reload latency (p95)
  hotReloadLatencyP95: 100ms
  hotReloadLatencyP99: 200ms

  # API response time (p95)
  apiResponseTimeP95: 50ms
  apiResponseTimeP99: 100ms

  # Error rate (max)
  errorRateMax: 0.01  # 1%

  # Availability (min)
  availabilityMin: 0.999  # 99.9%

  # Throughput (min RPS)
  throughputMin: 40

  # Multi-region replication lag (p95)
  replicationLagP95: 3s

  # Regression detection (F1 min)
  regressionDetectionF1Min: 0.85

# =====================================================================
# CANARY DEPLOYMENT
# =====================================================================
canary:
  enabled: true
  steps:
    - 5
    - 25
    - 50
    - 100
  interval: 10m  # 10 minutes per step
  threshold:
    successRate: 0.95
    errorRate: 0.05
    latencyP95: 100ms
  autoRollback: true
  analysis:
    - metric: http_request_duration_seconds
      threshold: 0.1
      interval: 5m
    - metric: http_requests_total
      threshold: 0.95
      interval: 5m

# =====================================================================
# STATUSPAGE INTEGRATION
# =====================================================================
statuspage:
  enabled: true
  apiUrl: https://api.statuspage.io/v1
  pageId: tars-platform
  components:
    - id: eval-engine
      name: Evaluation Engine
    - id: hypersync
      name: HyperSync Service
    - id: rl-agents
      name: RL Agents
    - id: dashboard
      name: Dashboard
    - id: api
      name: API

  updateInterval: 60s

  incidents:
    autoCreate: true
    severity:
      critical: degraded
      high: degraded
      medium: partial_outage
      low: investigating

# =====================================================================
# NETWORK POLICIES
# =====================================================================
networkPolicies:
  enabled: true
  policyTypes:
    - Ingress
    - Egress

  # Default deny all
  defaultDeny: true

  # Allow specific traffic
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              name: ingress-nginx
      ports:
        - protocol: TCP
          port: 8080
        - protocol: TCP
          port: 3001
        - protocol: TCP
          port: 3000

  egress:
    - to:
        - namespaceSelector:
            matchLabels:
              name: tars-data
      ports:
        - protocol: TCP
          port: 5432
        - protocol: TCP
          port: 6379

    - to:
        - namespaceSelector:
            matchLabels:
              name: observability
      ports:
        - protocol: TCP
          port: 6831
        - protocol: TCP
          port: 4318

# =====================================================================
# SECURITY POLICIES
# =====================================================================
podSecurityPolicy:
  enabled: true
  allowPrivilegeEscalation: false
  runAsNonRoot: true
  readOnlyRootFilesystem: true
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  fsGroup:
    rule: RunAsAny
  volumes:
    - configMap
    - emptyDir
    - projected
    - secret
    - downwardAPI

# =====================================================================
# MIGRATIONS (Database)
# =====================================================================
migrations:
  enabled: true
  image:
    repository: ghcr.io/veleron-dev-studios/tars-migrations
    tag: v1.0.0

  job:
    restartPolicy: OnFailure
    backoffLimit: 3

  annotations:
    argocd.argoproj.io/sync-wave: "0"
    argocd.argoproj.io/hook: PreSync
    argocd.argoproj.io/hook-delete-policy: BeforeHookCreation

# =====================================================================
# COST OPTIMIZATION
# =====================================================================
costOptimization:
  # Pod priority classes
  priorityClasses:
    critical: system-cluster-critical
    high: high-priority
    medium: medium-priority
    low: low-priority

  # Resource quotas per namespace
  resourceQuotas:
    pods: 100
    requestsCpu: "50"
    requestsMemory: "100Gi"
    limitsCpu: "100"
    limitsMemory: "200Gi"

# =====================================================================
# END OF PRODUCTION HELM VALUES
# =====================================================================
# Total: ~700 LOC
#
# Usage:
#   helm install tars charts/tars \
#     -f charts/tars/values.yaml \
#     -f deploy/ga/helm_values_ga.yaml \
#     --namespace tars-production
#
# Validate:
#   helm template tars charts/tars \
#     -f charts/tars/values.yaml \
#     -f deploy/ga/helm_values_ga.yaml | kubectl apply --dry-run=client -f -
# =====================================================================
